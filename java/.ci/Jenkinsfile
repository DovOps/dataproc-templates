def stageRetryCount = 3

pipeline {
    
    agent any

    environment {
        DATAPROC_TELEPORT_WEBHOOK_URL = credentials('dataproc-teleport-webhook-url')

        TEST_JDBC_URL = credentials('env-test-jdbc-url')

        GIT_BRANCH_LOCAL = sh (
            script: "echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'",  // Remove "origin/" and set the default branch to main
            returnStdout: true
        ).trim()
        
        MAVEN_HOME = "/var/lib/jenkins/tools/hudson.tasks.Maven_MavenInstallation/maven"
        PATH = "$PATH:$MAVEN_HOME/bin"

        GCS_STAGING_LOCATION = sh (script: '''
            CURRENT_BRANCH=`echo $branchName | sed -e 's|origin/||g' | sed -e 's|^null\$|main|'`
            if [ $CURRENT_BRANCH != "main" ];then
            echo "$GCS_STAGING_LOCATION/$(uuidgen)"
            else
            echo $GCS_STAGING_LOCATION
            fi
            '''.stripIndent(),
            returnStdout: true
        ).trim()
    }
    stages {
        stage('Prepare Environment'){
            parallel{
                stage('Checkout') {
                    steps{
                        git branch: "${GIT_BRANCH_LOCAL}", changelog: false, poll: false, url: 'https://github.com/GoogleCloudPlatform/dataproc-templates/'    
                    }
                }
                stage('Reset Resources'){
                    steps {
                            catchError {
                                sh '''
                                    gcloud pubsub topics publish pubsubtogcsv3 --message='{"Name": "NewMsg", "Age": 10}'
                                    gcloud pubsub topics publish test-pubsub-bq --message='{"Name": "Another message", "Age": 18}' 2> /dev/null || true

                                    gsutil rm -r gs://dataproc-templates/integration-testing/checkpoint/KAFKATOBQ 2> /dev/null || true
                                    gsutil rm -r gs://dataproc-templates/integration-testing/output/KAFKATOGCS 2> /dev/null || true
                                    gsutil rm -r gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream 2> /dev/null || true

                                '''
                            }
                    }
                }
                stage('Cluster Creation'){
                    when {
                        // Run this stage only if JOB_TYPE is not set to CLUSTER
                        expression { env.JOB_TYPE == "CLUSTER" }
                    }
                    steps{
                        sh '''
                            if gcloud dataproc clusters list --region=$REGION --project=$GCP_PROJECT | grep -q $CLUSTER; then
                                echo "Cluster $CLUSTER already exists."
                            else
                                echo "Cluster $CLUSTER does not exist. Creating now..."
                                gcloud dataproc clusters create $CLUSTER \
                                --region $REGION \
                                --subnet $SUBNET \
                                --no-address \
                                --master-machine-type n1-standard-2 \
                                --master-boot-disk-size 500 \
                                --num-workers 2 \
                                --worker-machine-type n1-standard-2 \
                                --worker-boot-disk-size 500 \
                                --image-version 2.1-debian11 \
                                --optional-components ZOOKEEPER \
                                --max-idle 1800s \
                                --project $GCP_PROJECT
                            fi
                        '''
                    }
                }
            }
        }   
        //Deploy one time so that build is copied to GCS location
        stage('JDBC TO BQ'){
            steps {
                retry(count: stageRetryCount) {
                    sh '''
                        export JARS=gs://dataproc-templates/jars/mysql-connector-java.jar
                        
                        cd java
                        
                        bin/start.sh \
                        -- --template JDBCTOBIGQUERY \
                        --templateProperty jdbctobq.bigquery.location=$GCP_PROJECT.dataproc_templates.jdbctobq \
                        --templateProperty jdbctobq.jdbc.url="$TEST_JDBC_URL" \
                        --templateProperty jdbctobq.jdbc.driver.class.name=com.mysql.jdbc.Driver \
                        --templateProperty jdbctobq.sql="select * from test.employee" \
                        --templateProperty jdbctobq.write.mode=overwrite \
                        --templateProperty jdbctobq.temp.gcs.bucket=dataproc-templates/integration-testing/output/JDBCTOBIGQUERY 
                    '''
                }
            }
        }
        stage('Parallel Execution 10'){
            parallel{
                stage('PubSubLite To BigTable') {
                    steps{
                        retry(count: stageRetryCount) {
                            sh '''
                                export SKIP_BUILD=true
                                cd java
                                bin/start.sh \
                                -- --template PUBSUBLITETOBIGTABLE \
                                --templateProperty pubsublite.input.project.id=$GCP_PROJECT \
                                --templateProperty pubsublite.input.subscription=projects/$GCP_PROJECT_NUMBER/locations/us-west1/subscriptions/pubsublitetobigtable-test-subscription2 \
                                --templateProperty pubsublite.checkpoint.location=dataproc-template-test1/checkpointlocation \
                                --templateProperty pubsublite.bigtable.output.project.id=$GCP_PROJECT \
                                --templateProperty pubsublite.bigtable.output.instance.id=$ENV_TEST_BIGTABLE_INSTANCE \
                                --templateProperty pubsublite.bigtable.output.table=bus-data
                            '''
                        }
                    }
                }
                stage('KAFKA TO BQ via Dstream'){
                     steps {
                         retry(count: stageRetryCount) {
                             sh '''

                                  export SKIP_BUILD=true
                                  cd java
                                  bin/start.sh \
                                  -- \
                                  --template KafkaToBQDstream \
                                  --templateProperty project.id=$GCP_PROJECT \
                                  --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                  --templateProperty kafka.topic=integration-test-kafka-bq-dstream \
                                  --templateProperty kafka.bq.dataset=kafkatobq \
                                  --templateProperty kafka.starting.offset=latest \
                                  --templateProperty kafka.bq.batch.interval=60000 \
                                  --templateProperty kafka.bq.consumer.group.id=testgroup \
                                  --templateProperty kafka.bq.stream.output.mode=append \
                                  --templateProperty kafka.bq.table=integration-test \
                                  --templateProperty kafka.bq.temp.gcs.bucket=dataproc-templates-kafkatobq \
                                  --templateProperty kafka.bq.await.termination.timeout=60000
                              '''
                         }
                     }
                }
                stage('KAFKA TO GCS via Dstream (bytes)'){
                     steps {
                         retry(count: stageRetryCount) {
                             sh '''

                                  export SKIP_BUILD=true
                                  cd java
                                  bin/start.sh \
                                  -- \
                                  --template KafkaToGCSDstream \
                                  --templateProperty project.id=$GCP_PROJECT \
                                  --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                  --templateProperty kafka.topic=integration-test-kafka-gcs-dstream \
                                  --templateProperty kafka.starting.offset=latest \
                                  --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream/bytes \
                                  --templateProperty kafka.gcs.batch.interval=60000 \
                                  --templateProperty kafka.gcs.consumer.group.id=testgroupgcs \
                                  --templateProperty kafka.gcs.write.mode=append \
                                  --templateProperty kafka.message.format=bytes \
                                  --templateProperty kafka.gcs.output.format=parquet \
                                  --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                             '''
                         }
                     }
                }
                stage('KAFKA TO GCS via Dstream (json)'){
                    steps {
                        retry(count: stageRetryCount) {
                            sh '''

                                 export SKIP_BUILD=true
                                 cd java
                                 bin/start.sh \
                                 -- \
                                 --template KafkaToGCSDstream \
                                 --templateProperty project.id=$GCP_PROJECT \
                                 --templateProperty kafka.bootstrap.servers=$ENV_TEST_KAFKA_BOOTSTRAP_SERVERS \
                                 --templateProperty kafka.topic=integration-test-json-kafka-gcs-dstream \
                                 --templateProperty kafka.starting.offset=latest \
                                 --templateProperty kafka.gcs.output.location=gs://dataproc-templates/integration-testing/output/KAFKATOGCS_DStream/json \
                                 --templateProperty kafka.gcs.batch.interval=60000 \
                                 --templateProperty kafka.gcs.consumer.group.id=testgroupjson \
                                 --templateProperty kafka.gcs.write.mode=append \
                                 --templateProperty kafka.message.format=json \
                                 --templateProperty kafka.schema.url=gs://dataproc-templates/integration-testing/schema.json \
                                 --templateProperty kafka.gcs.output.format=parquet \
                                 --templateProperty kafka.gcs.await.termination.timeout.ms=60000
                            '''
                        }
                    }
                }
            }
        }
    }
    post {
        always{
            script {
                if( env.GIT_BRANCH_LOCAL == 'main' ){
                    googlechatnotification url: DATAPROC_TELEPORT_WEBHOOK_URL,
    				message: 'Jenkins: ${JOB_NAME}\nBuild status is ${BUILD_STATUS}\nSee ${BUILD_URL}\n',
    				notifyFailure: 'true',
    				notifyAborted: 'true',
    				notifyUnstable: 'true',
    				notifyNotBuilt: 'true',
    				notifyBackToNormal: 'true'
                }
            }
            catchError {
                sh '''
                if [ $GIT_BRANCH_LOCAL != "main" ];then
                    gsutil rm -r $GCS_STAGING_LOCATION 2> /dev/null || true
                fi
                '''
            }
        }
    }
}