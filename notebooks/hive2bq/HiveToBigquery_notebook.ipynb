{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3f55fbb-2197-4038-88c5-6c896a9f071a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9eff8d-6b63-4e8f-b369-68cbb4ef04ee",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html)\n",
    "- [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "- [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "- [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account.  \n",
    "Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852a764-dad3-4f3b-b0b5-a71825469fd3",
   "metadata": {},
   "source": [
    "#### Permissions\n",
    "\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "- roles/aiplatform.serviceAgent\n",
    "- roles/aiplatform.customCodeServiceAgent\n",
    "- roles/storage.objectCreator\n",
    "- roles/storage.objectViewer\n",
    "- roles/dataproc.editor\n",
    "- roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640fa29-1a04-4301-974d-9fcec95b7e7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1:\n",
    "#### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3742b9a-143d-49fc-b43c-e56179c7f0f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.3.1)\n",
      "Requirement already satisfied: install in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: google-auth==2.13.0 in /opt/conda/lib/python3.7/site-packages (2.13.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth==2.13.0) (0.2.8)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth==2.13.0) (0.4.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: google-cloud-bigquery-migration in /opt/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-migration) (3.19.6)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-migration) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery-migration) (1.22.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (2.13.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (1.58.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (1.51.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (1.47.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (0.2.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-migration) (0.4.8)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth (/opt/conda/lib/python3.7/site-packages)\u001b[0m\u001b[33m\n",
      "Hit:1 http://security.debian.org/debian-security buster/updates InRelease\n",
      "Hit:2 http://packages.cloud.google.com/apt cloud-sdk-buster InRelease\n",
      "Hit:3 http://packages.cloud.google.com/apt google-cloud-packages-archive-keyring-buster InRelease\n",
      "Hit:4 http://deb.debian.org/debian buster InRelease                            \n",
      "Hit:5 http://deb.debian.org/debian buster-updates InRelease                    \n",
      "Hit:6 http://deb.debian.org/debian buster-backports InRelease                  \n",
      "Hit:7 http://packages.cloud.google.com/apt gcsfuse-buster InRelease            \n",
      "Get:8 https://download.docker.com/linux/debian buster InRelease [54.0 kB]      \n",
      "Hit:10 http://packages.cloud.google.com/apt google-compute-engine-buster-stable InRelease\n",
      "Get:9 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [8993 B]\n",
      "Fetched 63.0 kB in 1s (46.1 kB/s)  \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "default-jdk is already the newest version (2:1.11-71).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Google Cloud notebooks requires dependencies to be installed with '--user'\n",
    "! pip3 install pyspark\n",
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q\n",
    "! pip3 install pip install google-auth==2.13.0\n",
    "! pip3 install google-cloud-bigquery-migration\n",
    "# Install latest JDK\n",
    "! sudo apt-get update\n",
    "! sudo apt-get install default-jdk -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58552a67-9012-4ba9-82e9-d34299cd6d15",
   "metadata": {},
   "source": [
    "#### Once you've installed the additional packages, you may need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "Uncomment & Run this cell if you have installed anything from above commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01c19b5e-e7d9-416f-ad12-edc19b6877e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#    import IPython\n",
    "#    app = IPython.Application.instance()\n",
    "#    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2fa95e-b745-4649-8040-af99e7a4013c",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "#### Set Google Cloud properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dd937-7a43-4709-8297-f9c346d7897c",
   "metadata": {},
   "source": [
    "**Overview**  \n",
    "This notebook shows how to build a Vertex AI Pipeline to run a Dataproc Template   \n",
    "using the DataprocPySparkBatchOp component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e2ef85e-4cac-464c-9ed4-a66ea9c4f4c6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User Configuration\n",
    "# User Inputs\n",
    "\n",
    "get_project_id = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = get_project_id[0]\n",
    "REGION = \"\"  # example \"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://<bucket_name>\" # example \"gs://my_bucket_name\"\n",
    "SUBNET = \"\" # example \"projects/<project-id>/regions/<region-id>/subnetworks/<subnet-name>\" \n",
    "INPUT_HIVE_DATABASE= \"\"\n",
    "INPUT_HIVE_TABLES= \"*\" # example \"table1,table2,table3...\" or \"*\"\n",
    "OUTPUT_BIGQUERY_DATASET= \"\"\n",
    "TEMP_BUCKET= \"\"\n",
    "HIVE_METASTORE= \"\" # example \"thrift://hive-cluster-m:9083\"\n",
    "\n",
    "## Change if needed\n",
    "HIVE_OUTPUT_MODE=\"overwrite\"\n",
    "MAX_PARALLELISM=10 # Controlls number of parallel Dataproc Serverless Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454bb0de-3e0a-4daa-92ed-1319e1d9604d",
   "metadata": {},
   "source": [
    "#### Step 3:\n",
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7aede36-3a0a-43f7-8e4c-db2d8087e289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "from google_cloud_pipeline_components.experimental.dataproc import DataprocPySparkBatchOp\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from utils.hive_notebook_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b33331-b529-47ac-a08d-0c166acd264e",
   "metadata": {},
   "source": [
    "#### Step 4:\n",
    "#### Change working directory to the Dataproc Templates python folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6e0e80a-e8b7-4e54-9f99-b7874e164978",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dataproc-templates/python\n"
     ]
    }
   ],
   "source": [
    "WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python\"\n",
    "%cd /home/jupyter/dataproc-templates/python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bb252-c3e3-4d56-b1ad-f52a2db52869",
   "metadata": {},
   "source": [
    "#### Step 5:\n",
    "#### Build Dataproc Templates python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfaa8c93-5e69-48fc-984a-f4fa3b28519b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_egg\n",
      "running egg_info\n",
      "writing google_dataproc_templates.egg-info/PKG-INFO\n",
      "writing dependency_links to google_dataproc_templates.egg-info/dependency_links.txt\n",
      "writing requirements to google_dataproc_templates.egg-info/requires.txt\n",
      "writing top-level names to google_dataproc_templates.egg-info/top_level.txt\n",
      "reading manifest file 'google_dataproc_templates.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'google_dataproc_templates.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/test\n",
      "creating build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/__init__.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_text_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "creating build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/test_cassandra_to_bq.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "copying build/lib/test/cassandra/__init__.py -> build/bdist.linux-x86_64/egg/test/cassandra\n",
      "creating build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/test_hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/__init__.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "creating build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "creating build/bdist.linux-x86_64/egg/test/redshift\n",
      "copying build/lib/test/redshift/__init__.py -> build/bdist.linux-x86_64/egg/test/redshift\n",
      "copying build/lib/test/redshift/test_redshift_to_gcs.py -> build/bdist.linux-x86_64/egg/test/redshift\n",
      "creating build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/__init__.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "creating build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/test_bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "creating build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/__init__.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/test_argument_parsing.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "creating build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/test_mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/__init__.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/text_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/cassandra_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "copying build/lib/dataproc_templates/cassandra/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/cassandra\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/base_template.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/redshift/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/redshift/redshift_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/redshift\n",
      "copying build/lib/dataproc_templates/template_name.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "copying build/lib/dataproc_templates/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/argument_parsing.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/template_constants.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/tracking.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "copying build/lib/dataproc_templates/snowflake/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "copying build/lib/dataproc_templates/snowflake/snowflake_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/snowflake\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_mongo.py to test_gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_gcs.py to test_gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigtable.py to test_gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_text_to_bigquery.py to test_text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigquery.py to test_gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_jdbc.py to test_gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/test_cassandra_to_bq.py to test_cassandra_to_bq.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/cassandra/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/test_hbase_to_gcs.py to test_hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_jdbc.py to test_jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_gcs.py to test_jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_bigquery.py to test_jdbc_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/redshift/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/redshift/test_redshift_to_gcs.py to test_redshift_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_bigquery.py to test_hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_gcs.py to test_hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/test_bigquery_to_gcs.py to test_bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/test_argument_parsing.py to test_argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/test_mongo_to_gcs.py to test_mongo_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_mongo.py to gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/text_to_bigquery.py to text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigquery.py to gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigtable.py to gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_jdbc.py to gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_gcs.py to gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/cassandra_to_bigquery.py to cassandra_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/cassandra/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/hbase_to_gcs.py to hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_gcs.py to jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_bigquery.py to jdbc_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_jdbc.py to jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/base_template.py to base_template.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/redshift/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/redshift/redshift_to_gcs.py to redshift_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/template_name.py to template_name.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_bigquery.py to hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_gcs.py to hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/bigquery_to_gcs.py to bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/argument_parsing.py to argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/template_constants.py to template_constants.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/tracking.py to tracking.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/snowflake/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/snowflake/snowflake_to_gcs.py to snowflake_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/mongo_to_gcs.py to mongo_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying google_dataproc_templates.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/google_dataproc_templates-0.0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Will rename output .egg file from dist/google_dataproc_templates-0.0.1-py3.7.egg to dist/dataproc_templates_distribution.egg\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_EGG_FILE = \"dist/dataproc_templates_distribution.egg\"\n",
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528c874-b948-40c1-b57c-afff76d80b47",
   "metadata": {},
   "source": [
    "#### Step 6:\n",
    "#### Copy package to the GCS bucket\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/storage.objectCreator\n",
    " - roles/storage.objectViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33dd85ce-46f6-4b35-8997-2189cf1b2eee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://main.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  4.9 KiB/  4.9 KiB]                                                \n",
      "Operation completed over 1 objects/4.9 KiB.                                      \n",
      "Copying file://dist/dataproc_templates_distribution.egg [Content-Type=application/octet-stream]...\n",
      "/ [1 files][155.4 KiB/155.4 KiB]                                                \n",
      "Operation completed over 1 objects/155.4 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "!gsutil cp $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/dist/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d8c5f-b28b-4fbb-b585-cb2f1d9c1915",
   "metadata": {},
   "source": [
    "#### Step 7:\n",
    "#### Get Hive Tables \n",
    "In case user wants to load all the Hive tables from the database, we need to get the table list using the metastore.\n",
    "\n",
    "Below cell will fetch all tables from the Hive database by running a Spark SQL query using the provided Hive Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3751abe3-47b7-4b4a-81f4-498da750d884",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Sets to Migrate: \n",
      "['student', 'student2', 'student3', 'student4', 'studentm']\n"
     ]
    }
   ],
   "source": [
    "spark=get_spark_session(HIVE_METASTORE)\n",
    "\n",
    "if INPUT_HIVE_TABLES==\"*\":\n",
    "    #os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "    #os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "    TABLE_LIST_DF=spark.sql(\"show tables in \"+INPUT_HIVE_DATABASE)\n",
    "    TABLE_LIST=TABLE_LIST_DF.select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    print(\"Table Sets to Migrate: \")\n",
    "    print(TABLE_LIST)\n",
    "else:\n",
    "    TABLE_LIST=INPUT_HIVE_TABLES.split(\",\")\n",
    "    print(\"Table Sets to Migrate: \")\n",
    "    print(TABLE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2e0cc-62fb-4ecf-8eaf-c16565fee1cc",
   "metadata": {},
   "source": [
    "#### Step 8:\n",
    "#### Extract Hive DDls \n",
    "Below cell will fetch DDls of all the tables in the given database from HIVE metastore and store in the below GCP location\n",
    "\n",
    "gs://{TEMP_BUCKET}/hive_ddls/input/{INPUT_HIVE_DATABASE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c4f1a2e-e75c-4583-a22e-8972b9decaaf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partitiondb\n",
      "student\n",
      "CREATE TABLE partitiondb.student (\n",
      "  student_name STRING,\n",
      "  class_name STRING,\n",
      "  percentage FLOAT)\n",
      "PARTITIONED BY (section STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = ',',\n",
      "  'field.delim' = ',')\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "TBLPROPERTIES (\n",
      "  'bucketing_version' = '2',\n",
      "  'transient_lastDdlTime' = '1658332738')\n",
      "\n",
      "partitiondb\n",
      "student2\n",
      "CREATE TABLE partitiondb.student2 (\n",
      "  student_name STRING,\n",
      "  class_name STRING,\n",
      "  percentage FLOAT)\n",
      "PARTITIONED BY (datenow DATE)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = ',',\n",
      "  'field.delim' = ',')\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "TBLPROPERTIES (\n",
      "  'bucketing_version' = '2',\n",
      "  'transient_lastDdlTime' = '1674919365')\n",
      "\n",
      "partitiondb\n",
      "student3\n",
      "CREATE TABLE partitiondb.student3 (\n",
      "  student_name STRING,\n",
      "  class_name STRING,\n",
      "  percentage FLOAT,\n",
      "  datenow DATE)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = ',',\n",
      "  'field.delim' = ',')\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "TBLPROPERTIES (\n",
      "  'bucketing_version' = '2',\n",
      "  'transient_lastDdlTime' = '1674928644')\n",
      "\n",
      "partitiondb\n",
      "student4\n",
      "CREATE TABLE partitiondb.student4 (\n",
      "  student_name STRING,\n",
      "  class_name STRING,\n",
      "  percentage FLOAT,\n",
      "  datenow DATE)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1')\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "TBLPROPERTIES (\n",
      "  'bucketing_version' = '2',\n",
      "  'transient_lastDdlTime' = '1674928713')\n",
      "\n",
      "partitiondb\n",
      "studentm\n",
      "CREATE TABLE partitiondb.studentm (\n",
      "  studentname STRING,\n",
      "  classname STRING,\n",
      "  percentage FLOAT)\n",
      "PARTITIONED BY (doj DATE, section STRING, groupnum INT)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = ',',\n",
      "  'field.delim' = ',')\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "TBLPROPERTIES (\n",
      "  'bucketing_version' = '2',\n",
      "  'transient_lastDdlTime' = '1658342923')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract HIVE DDLs from Hive Metastore\n",
    "get_hive_ddls(INPUT_HIVE_DATABASE,TABLE_LIST,TEMP_BUCKET,spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa5d3f-cd89-4855-84c3-87e99b10dfc5",
   "metadata": {},
   "source": [
    "#### Step 9:\n",
    "#### Create BQ translation Migration Workfloe \n",
    "Below cell will use BQ translation API to convert all the DDLs from HIVE to BQ syntax. The translation API is intelligent enough to identify the required columns to be used as partitioned or clustered columns in BQ tables.\n",
    "\n",
    "All the converted queries can be found at:\n",
    "\n",
    "gs://{TEMP_BUCKET}/hive_ddls/output/{INPUT_HIVE_DATABASE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a73de1a-a110-4a3a-8231-520cb3576277",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created workflow: demo-workflow-python-example-Hive2BQ\n",
      "Current state: RUNNING\n",
      "Current state: RUNNING\n",
      "Current state: RUNNING\n",
      "Current state: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "# Run BQ translation workflow to convert HIVE DDls to BQ\n",
    "GCS_INPUT_PATH=f\"gs://{TEMP_BUCKET}/hive_ddls/input/{INPUT_HIVE_DATABASE}\"\n",
    "GCS_OUTPUT_PATH=f\"gs://{TEMP_BUCKET}/hive_ddls/output/{INPUT_HIVE_DATABASE}\"\n",
    "create_migration_workflow(GCS_INPUT_PATH,GCS_OUTPUT_PATH,PROJECT_ID,OUTPUT_BIGQUERY_DATASET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2812484-0cda-413e-bb6f-e616a81c4583",
   "metadata": {},
   "source": [
    "#### Step 10:\n",
    "#### Create BQ tables: \n",
    "Below cell will create empty BQ tables in Bigquery with partitioned and clustered keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c6e562f-0f31-41c4-8db8-0797cb2ae080",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryJob<project=yadavaja-sandbox, location=US, id=8dbdb2ec-3668-43dd-880a-26aac94151ef>\n"
     ]
    }
   ],
   "source": [
    "# Create BQ tables\n",
    "create_bq_tables(TEMP_BUCKET,INPUT_HIVE_DATABASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbaf266-91a0-46e2-8a88-b594b9301c83",
   "metadata": {},
   "source": [
    "#### Step 11:\n",
    "\n",
    "Split Hive Tables list based on MAX_PARALLELISM value provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b170e58-7acf-4aae-9619-2361eef0ed12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tables for execution : \n",
      "[['student', 'student2', 'student3', 'student4', 'studentm']]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "COMPLETE_LIST = copy.deepcopy(TABLE_LIST)\n",
    "PARALLEL_JOBS = len(TABLE_LIST)//MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST)>0 :\n",
    "            SUB_LIST.append(COMPLETE_LIST[0])\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"List of tables for execution : \")\n",
    "print(JOB_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5a0b9-d2b2-4072-aa4f-3748609f7cdc",
   "metadata": {},
   "source": [
    "#### Step 12:\n",
    "\n",
    "Set Dataproc Template Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73197165-0ea0-4732-841d-8ba4bcd8a94d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "JARS = [\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"]\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dist/dataproc_templates_distribution.egg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332168ba-6e69-4f6e-aa03-38b4965b5304",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 13:\n",
    "#### Build pipeline and run Dataproc Template on Vertex AI Pipelines to migrate Hive tables to BigQuery\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/dataproc.editor\n",
    " - roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "095c13bf-c121-4465-88fc-04778ef35a36",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "runtime_prop={}\n",
    "runtime_prop['spark.hadoop.hive.metastore.uris']=HIVE_METASTORE\n",
    "runtime_prop['mapreduce.fileoutputcommitter.marksuccessfuljobs'] = \"false\"\n",
    "\n",
    "def migrate_hive(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name=\"hive-to-bq-pyspark\",\n",
    "        description=\"Pipeline to migrate tables from hive to bq\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        location: str = REGION,\n",
    "        main_python_file_uri: str = MAIN_PYTHON_FILE,\n",
    "        python_file_uris: list = PYTHON_FILE_URIS,\n",
    "        jar_file_uris: list = JARS,\n",
    "        subnetwork_uri: str = SUBNET\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"hive2bq-{}-{}\".format(table,datetime.now().strftime(\"%s\")).replace('_','-')\n",
    "            TEMPLATE_SPARK_ARGS = [\n",
    "                                    \"--template=HIVETOBIGQUERY\",\n",
    "                                    \"--hive.bigquery.input.database={}\".format(INPUT_HIVE_DATABASE),\n",
    "                                    \"--hive.bigquery.input.table={}\".format(table),\n",
    "                                    \"--hive.bigquery.output.table={}\".format(table),\n",
    "                                    \"--hive.bigquery.output.dataset={}\".format(OUTPUT_BIGQUERY_DATASET),\n",
    "                                    \"--hive.bigquery.output.mode={}\".format(HIVE_OUTPUT_MODE),\n",
    "                                    \"--hive.bigquery.temp.bucket.name={}\".format(TEMP_BUCKET)                                    ]\n",
    "            _ = DataprocPySparkBatchOp(\n",
    "                project=project_id,\n",
    "                location=location,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_python_file_uri=main_python_file_uri,\n",
    "                python_file_uris=python_file_uris,\n",
    "                jar_file_uris=jar_file_uris,\n",
    "                subnetwork_uri=subnetwork_uri,\n",
    "                runtime_config_properties=runtime_prop,\n",
    "                args=TEMPLATE_SPARK_ARGS,\n",
    "            )\n",
    "            time.sleep(5)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "            display_name=\"pipeline\",\n",
    "            template_path=\"pipeline.json\",\n",
    "            pipeline_root=PIPELINE_ROOT,\n",
    "            enable_caching=False,\n",
    "            )\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f455a-f606-4d7c-aceb-e75448052313",
   "metadata": {},
   "source": [
    "#### Step 14:\n",
    "\n",
    "#### If the user wants to verify DDLs first, uncomment the below line to stop execution of the final cell which loads the data to BQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "73ccb704-338c-4fa3-95d5-e0051398bb96",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # don't go beyond here with Run All\n",
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba4453-ddf9-4f35-901e-ab20433ae7fa",
   "metadata": {},
   "source": [
    "#### Step 15:\n",
    "\n",
    "Run Dataproc Batch Template based on Hive Tables list calculated in Step 8.\n",
    "\n",
    "The below cell will call function migrate_hive to migrate tables using dataproc serverless batch job and also add an entry in Audit Table for each Table Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4580bcc2-9302-48dc-8a47-7ccac603fbd7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading Table Set: ['student', 'student2', 'student3', 'student4', 'studentm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob created. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline_job = aiplatform.PipelineJob.get('projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/hive-to-bq-pyspark-20230129120952?project=617357862702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/hive-to-bq-pyspark-20230129120952?project=617357862702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineJob run completed. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/hive-to-bq-pyspark-20230129120952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loaded Table Set: ['student', 'student2', 'student3', 'student4', 'studentm']\n",
      "  Source_DB_Name                             Source_Table_Set Target_DB_Name  \\\n",
      "0    partitiondb  student|student2|student3|student4|studentm       hivedemo   \n",
      "\n",
      "                              Target_Table_Set              Job_Start_Time  \\\n",
      "0  student|student2|student3|student4|studentm  2023-01-29 12:09:26.955648   \n",
      "\n",
      "                 Job_End_Time Job_Status  \n",
      "0  2023-01-29 12:14:55.097716       PASS  \n"
     ]
    }
   ],
   "source": [
    "AUDIT_DICT={}\n",
    "AUDIT_DF = pd.DataFrame(columns=[\"Source_DB_Name\",\"Source_Table_Set\",\"Target_DB_Name\",\"Target_Table_Set\",\"Job_Start_Time\",\"Job_End_Time\",\"Job_Status\"])\n",
    " \n",
    "for execution_list in JOB_LIST:\n",
    "    print(\"\\n\\nLoading Table Set: \"+str(execution_list))\n",
    "    AUDIT_DICT[\"Source_DB_Name\"]=INPUT_HIVE_DATABASE\n",
    "    AUDIT_DICT[\"Source_Table_Set\"]='|'.join(execution_list)\n",
    "    AUDIT_DICT[\"Target_DB_Name\"]=OUTPUT_BIGQUERY_DATASET\n",
    "    AUDIT_DICT[\"Target_Table_Set\"]='|'.join(execution_list)\n",
    "    AUDIT_DICT[\"Job_Start_Time\"]=str(datetime.now())\n",
    "    try:\n",
    "        migrate_hive(execution_list)\n",
    "    except Exception:\n",
    "        AUDIT_DICT[\"Job_Status\"]=\"FAIL\"\n",
    "        print(\"\\n\\nSome Error Occured while loading Table Set: \"+str(execution_list))\n",
    "    else:\n",
    "        AUDIT_DICT[\"Job_Status\"]=\"PASS\"\n",
    "        print(\"\\n\\nLoaded Table Set: \"+str(execution_list))\n",
    "\n",
    "    AUDIT_DICT[\"Job_End_Time\"]=str(datetime.now())\n",
    "    AUDIT_DF=AUDIT_DF.append(AUDIT_DICT, ignore_index = True)\n",
    "    \n",
    "if AUDIT_DF.empty:\n",
    "    print(\"Audit Dataframe is Empty\")\n",
    "else:\n",
    "    print(AUDIT_DF)\n",
    "    AUDIT_DF.to_csv(\"gs://\"+TEMP_BUCKET+\"/audit/audit_file_{}.csv\".format(str(datetime.now())),index=False,header = False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
