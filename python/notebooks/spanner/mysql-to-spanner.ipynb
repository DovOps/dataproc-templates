{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf6018e-9879-4196-9fa3-706145bc74bc",
   "metadata": {},
   "source": [
    "1.User to specify mysql connection.\n",
    "2.Generate list of tables from metadata. Alternatively, user should be able to supply list of tables.\n",
    "3.Identify current primary key column name, and partitioned read properties.\n",
    "4.Should generate logic for partitioned read, such that each partition read is <2 GB in size.\n",
    "5.Run JDBCToGCS (Python or Java template) for MySQL to GCS for export.\n",
    "6.Run GCSToSpanner (Java) for import into Spanner.\n",
    "7.Notebook should allow for both types of save modes i.e. appending data or overwrite\n",
    "8.Notebook should allow table schema generation if table does not exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89b301-5249-462a-97d8-986488b303fd",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef65ec2-ad6b-407f-a993-7cdf871bba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymysql SQLAlchemy\n",
    "# Google Cloud notebooks requires dependencies to be installed with '--user'\n",
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d01e33-9099-4d2e-b57e-575c3a998d84",
   "metadata": {},
   "source": [
    "### Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59c749-74a0-4c39-9135-afb92fb8942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703b502-1b41-44f1-bf21-41069255bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pymysql\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from google_cloud_pipeline_components.experimental.dataproc import DataprocPySparkBatchOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4a209-db59-42f6-bba7-30cd46b16bad",
   "metadata": {},
   "source": [
    "### Step 3: Assign Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0f037-e888-4479-a143-f06a39bd5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_ADDRESS = \"10.203.209.12\"\n",
    "PORT = \"3306\"\n",
    "USERNAME = \"root\"\n",
    "PASSWORD = \"####\"\n",
    "DATABASE = \"INFORMATION_SCHEMA\"\n",
    "TABLE_LIST = ['CHARACTER_SETS', 'COLLATIONS', 'COLLATION_CHARACTER_SET_APPLICABILITY', 'COLUMNS','TABLES'] # leave list empty for migrating complete database\n",
    "MYSQL_OUTPUT_GCS_LOCATION = \"gs://python-dataproc-templates/mysql-gcs-output\"\n",
    "MYSQL_OUTPUT_GCS_MODE = \"overwrite\"\n",
    "MYSQL_OUTPUT_GCS_FORMAT = \"csv\"\n",
    "MAX_PARALLELISM = 2\n",
    "PROJECT_ID = \"yadavaja-sandbox\"\n",
    "REGION = \"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://python-dataproc-templates-temp/mysql-to-spanner-staging\"\n",
    "SUBNET = \"projects/yadavaja-sandbox/regions/us-west1/subnetworks/test-subnet1\"\n",
    "JARS = [\"gs://datproc_template_nk/jars/mysql-connector-java-8.0.29.jar\"]\n",
    "\n",
    "# Please update below variables only whenrequired\n",
    "PYMYSQL_DRIVER = \"mysql+pymysql\"\n",
    "JDBC_DRIVER = \"com.mysql.cj.jdbc.Driver\"\n",
    "JDBC_URL = \"jdbc:mysql://{}:{}/{}?user={}&password={}\".format(IP_ADDRESS,PORT,DATABASE,USERNAME,PASSWORD)\n",
    "WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python/\"\n",
    "PACKAGE_EGG_FILE = \"dist/dataproc_templates_distribution.egg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c062b-5a91-4372-b440-5c37a12fbf87",
   "metadata": {},
   "source": [
    "### Step 4: Generate MySQL Table List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e362ac-30cd-4857-9e2a-0e9eb926e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(TABLE_LIST) == 0:\n",
    "    DB = sqlalchemy.create_engine(\n",
    "            sqlalchemy.engine.url.URL.create(\n",
    "                drivername=PYMYSQL_DRIVER,\n",
    "                username=USERNAME,\n",
    "                password=PASSWORD,\n",
    "                database=DATABASE,\n",
    "                host=IP_ADDRESS,\n",
    "                port=PORT\n",
    "              )\n",
    "            )\n",
    "    with DB.connect() as conn:\n",
    "        print(\"connected to database\")\n",
    "        results = DB.execute('show tables;').fetchall()\n",
    "        print(\"Total Tables = \", len(results))\n",
    "        for row in results:\n",
    "            TABLE_LIST.append(row[0])\n",
    "\n",
    "print(\"list of tables for migration :\")\n",
    "print(TABLE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5f841-a687-4723-a8e6-6e7e752ba36e",
   "metadata": {},
   "source": [
    "### Step 5: Create Package Egg file and Upload to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22220ae3-9fb4-471c-b5aa-f606deeca15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd $WORKING_DIRECTORY\n",
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cdcd5-0f3e-4f51-aa78-93d1976cb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "! gsutil cp -r $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/dist/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bb170-09c4-40d1-baaf-9e907f215889",
   "metadata": {},
   "source": [
    "### Step 6: Calculate Parallel Job for MySQL to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c501db0-c1fb-4a05-88b8-a7e546e2b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate parallel jobs:\n",
    "COMPLETE_LIST = copy.deepcopy(TABLE_LIST)\n",
    "PARALLEL_JOBS = len(TABLE_LIST)//MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST)>0 :\n",
    "            SUB_LIST.append(COMPLETE_LIST[0])\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"list of tables for execution : \")\n",
    "print(JOB_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6f83b-891a-4515-a1d6-f3406a25dc2a",
   "metadata": {},
   "source": [
    "### Step 7: Execute Pipeline to Migrate tables from MySQL to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d67895-ba8e-4876-b929-427d1ea7c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dist/dataproc_templates_distribution.egg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fa2d8-4ef7-4722-87c8-eec6c06f892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_mysql(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name=\"mysql-to-gcs-pyspark\",\n",
    "        description=\"Pipeline to get data from mysql to gcs\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        location: str = REGION,\n",
    "        main_python_file_uri: str = MAIN_PYTHON_FILE,\n",
    "        python_file_uris: list = PYTHON_FILE_URIS,\n",
    "        jar_file_uris: list = JARS,\n",
    "        subnetwork_uri: str = SUBNET\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"mysql2gcs-{}\".format(datetime.now().strftime(\"%s\"))\n",
    "            TEMPLATE_SPARK_ARGS = [\n",
    "            \"--template=JDBCTOGCS\",\n",
    "            \"--jdbctogcs.input.url={}\".format(JDBC_URL),\n",
    "            \"--jdbctogcs.input.driver={}\".format(JDBC_DRIVER),\n",
    "            \"--jdbctogcs.input.table={}\".format(table),\n",
    "            \"--jdbctogcs.output.location={}/{}\".format(MYSQL_OUTPUT_GCS_LOCATION,table.lower()),\n",
    "            \"--jdbctogcs.output.mode={}\".format(MYSQL_OUTPUT_GCS_MODE),\n",
    "            \"--jdbctogcs.output.format={}\".format(MYSQL_OUTPUT_GCS_FORMAT)\n",
    "            ]\n",
    "            print(TEMPLATE_SPARK_ARGS)\n",
    "            _ = DataprocPySparkBatchOp(\n",
    "                project=project_id,\n",
    "                location=location,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_python_file_uri=main_python_file_uri,\n",
    "                python_file_uris=python_file_uris,\n",
    "                jar_file_uris=jar_file_uris,\n",
    "                subnetwork_uri=subnetwork_uri,\n",
    "                args=TEMPLATE_SPARK_ARGS,\n",
    "            )\n",
    "            time.sleep(3)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "            display_name=\"pipeline\",\n",
    "            template_path=\"pipeline.json\",\n",
    "            pipeline_root=PIPELINE_ROOT,\n",
    "            enable_caching=False,\n",
    "            )\n",
    "    pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44205b54-1ac7-42f3-85ad-5b20f531056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for execution_list in JOB_LIST:\n",
    "    print(execution_list)\n",
    "    migrate_mysql(execution_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc269506-e87e-4a9d-98e4-c22cfbcddb0b",
   "metadata": {},
   "source": [
    "4. Identify current primary key column name, and partitioned read properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c02cf5-0563-4a37-9c28-b07698ccef6b",
   "metadata": {},
   "source": [
    "5. Should generate logic for partitioned read, such that each partition read is <2 GB in size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c1884-a975-4c7e-8ec5-b34016fb16c1",
   "metadata": {},
   "source": [
    "7. create spanner schema if it does not exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be084c-1ef2-4893-a23f-c00811654871",
   "metadata": {},
   "source": [
    "8. get gcs to spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a81c31-c602-4e35-814a-4f74e7fcc447",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_SPARK_ARGS = [\n",
    "\"--template=JDBCTOGCS\",\n",
    "\"--jdbctogcs.input.url={}\".format(JDBC_URL),\n",
    "\"--jdbctogcs.input.driver={}\".format(JDBC_DRIVER),\n",
    "\"--jdbctogcs.input.table={}\".format(),\n",
    "\"--jdbctogcs.output.location={}/{}\".format(MYSQL_OUTPUT_GCS_LOCATION),\n",
    "\"--jdbctogcs.output.mode={}\".format(MYSQL_OUTPUT_GCS_MODE),\n",
    "\"--jdbctogcs.output.format={}\".format(MYSQL_OUTPUT_GCS_FORMAT)\n",
    "]\n",
    "# --jdbctogcs.input.partitioncolumn=\"id\" \\\n",
    "# --jdbctogcs.input.lowerbound=\"11\" \\\n",
    "# --jdbctogcs.input.upperbound=\"20\" \\\n",
    "# --jdbctogcs.numpartitions=\"4\" \\\n",
    "# --jdbctogcs.output.partitioncolumn=\"department_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fc60f-5cad-4a11-afb2-9d67250bf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my sql to gcs pending\n",
    "#3.Identify current primary key column name, and partitioned read properties. \n",
    "#4.Should generate logic for partitioned read, such that each partition read is <2 GB in size\n",
    "# lower bound, upper bound partition\n",
    "# parallel execution"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
