{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf6018e-9879-4196-9fa3-706145bc74bc",
   "metadata": {},
   "source": [
    "1.User to specify mysql connection.\n",
    "2.Generate list of tables from metadata. Alternatively, user should be able to supply list of tables.\n",
    "3.Identify current primary key column name, and partitioned read properties.\n",
    "4.Should generate logic for partitioned read, such that each partition read is <2 GB in size.\n",
    "5.Run JDBCToGCS (Python or Java template) for MySQL to GCS for export.\n",
    "6.Run GCSToSpanner (Java) for import into Spanner.\n",
    "7.Notebook should allow for both types of save modes i.e. appending data or overwrite\n",
    "8.Notebook should allow table schema generation if table does not exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89b301-5249-462a-97d8-986488b303fd",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef65ec2-ad6b-407f-a993-7cdf871bba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in /opt/conda/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: SQLAlchemy in /opt/conda/lib/python3.7/site-packages (1.4.39)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from SQLAlchemy) (4.11.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from SQLAlchemy) (1.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->SQLAlchemy) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->SQLAlchemy) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql SQLAlchemy\n",
    "# Google Cloud notebooks requires dependencies to be installed with '--user'\n",
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d01e33-9099-4d2e-b57e-575c3a998d84",
   "metadata": {},
   "source": [
    "### Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b59c749-74a0-4c39-9135-afb92fb8942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2703b502-1b41-44f1-bf21-41069255bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pymysql\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime\n",
    "import time\n",
    "import copy\n",
    "from google_cloud_pipeline_components.experimental.dataproc import DataprocPySparkBatchOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4a209-db59-42f6-bba7-30cd46b16bad",
   "metadata": {},
   "source": [
    "### Step 3: Assign Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f0f037-e888-4479-a143-f06a39bd5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_ADDRESS = \"10.203.209.12\"\n",
    "PORT = \"3306\"\n",
    "USERNAME = \"root\"\n",
    "PASSWORD = \"####\"\n",
    "DATABASE = \"INFORMATION_SCHEMA\"\n",
    "TABLE_LIST = ['CHARACTER_SETS', 'COLLATIONS', 'COLLATION_CHARACTER_SET_APPLICABILITY', 'COLUMNS','TABLES'] # leave list empty for migrating complete database\n",
    "MYSQL_OUTPUT_GCS_LOCATION = \"gs://python-dataproc-templates/mysql-gcs-output\"\n",
    "MYSQL_OUTPUT_GCS_MODE = \"overwrite\"\n",
    "MYSQL_OUTPUT_GCS_FORMAT = \"csv\"\n",
    "MAX_PARALLELISM = 2\n",
    "PROJECT_ID = \"yadavaja-sandbox\"\n",
    "REGION = \"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://python-dataproc-templates-temp/mysql-to-spanner-staging\"\n",
    "SUBNET = \"projects/yadavaja-sandbox/regions/us-west1/subnetworks/test-subnet1\"\n",
    "JARS = [\"gs://datproc_template_nk/jars/mysql-connector-java-8.0.29.jar\"]\n",
    "\n",
    "# Please update below variables only whenrequired\n",
    "PYMYSQL_DRIVER = \"mysql+pymysql\"\n",
    "JDBC_DRIVER = \"com.mysql.cj.jdbc.Driver\"\n",
    "JDBC_URL = \"jdbc:mysql://{}:{}/{}?user={}&password={}\".format(IP_ADDRESS,PORT,DATABASE,USERNAME,PASSWORD)\n",
    "WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python/\"\n",
    "PACKAGE_EGG_FILE = \"dist/dataproc_templates_distribution.egg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c062b-5a91-4372-b440-5c37a12fbf87",
   "metadata": {},
   "source": [
    "### Step 4: Generate MySQL Table List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e362ac-30cd-4857-9e2a-0e9eb926e627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of tables for migration :\n",
      "['CHARACTER_SETS', 'COLLATIONS', 'COLLATION_CHARACTER_SET_APPLICABILITY', 'COLUMNS', 'TABLES']\n"
     ]
    }
   ],
   "source": [
    "if len(TABLE_LIST) == 0:\n",
    "    DB = sqlalchemy.create_engine(\n",
    "            sqlalchemy.engine.url.URL.create(\n",
    "                drivername=PYMYSQL_DRIVER,\n",
    "                username=USERNAME,\n",
    "                password=PASSWORD,\n",
    "                database=DATABASE,\n",
    "                host=IP_ADDRESS,\n",
    "                port=PORT\n",
    "              )\n",
    "            )\n",
    "    with DB.connect() as conn:\n",
    "        print(\"connected to database\")\n",
    "        results = DB.execute('show tables;').fetchall()\n",
    "        print(\"Total Tables = \", len(results))\n",
    "        for row in results:\n",
    "            TABLE_LIST.append(row[0])\n",
    "\n",
    "print(\"list of tables for migration :\")\n",
    "print(TABLE_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5f841-a687-4723-a8e6-6e7e752ba36e",
   "metadata": {},
   "source": [
    "### Step 5: Create Package Egg file and Upload to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22220ae3-9fb4-471c-b5aa-f606deeca15e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dataproc-templates/python\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing dataproc_templates.egg-info/PKG-INFO\n",
      "writing dependency_links to dataproc_templates.egg-info/dependency_links.txt\n",
      "writing requirements to dataproc_templates.egg-info/requires.txt\n",
      "writing top-level names to dataproc_templates.egg-info/top_level.txt\n",
      "reading manifest file 'dataproc_templates.egg-info/SOURCES.txt'\n",
      "writing manifest file 'dataproc_templates.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/test\n",
      "creating build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/__init__.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/test_hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "creating build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/test_bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "creating build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/__init__.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "creating build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/__init__.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_text_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "creating build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/__init__.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/test_argument_parsing.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "creating build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "copying build/lib/test/jdbc/test_jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/jdbc\n",
      "creating build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/test_mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "copying build/lib/test/mongo/__init__.py -> build/bdist.linux-x86_64/egg/test/mongo\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "copying build/lib/dataproc_templates/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "copying build/lib/dataproc_templates/template_name.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/text_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_mongo.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/base_template.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/template_constants.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/tracking.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/argument_parsing.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "copying build/lib/dataproc_templates/jdbc/jdbc_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/jdbc\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "copying build/lib/dataproc_templates/mongo/mongo_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/mongo\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/test_hbase_to_gcs.py to test_hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/test_bigquery_to_gcs.py to test_bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_bigquery.py to test_hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_gcs.py to test_hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_text_to_bigquery.py to test_text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_gcs.py to test_gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_mongo.py to test_gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigtable.py to test_gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigquery.py to test_gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_jdbc.py to test_gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/test_argument_parsing.py to test_argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_gcs.py to test_jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/jdbc/test_jdbc_to_jdbc.py to test_jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/test_mongo_to_gcs.py to test_mongo_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/template_name.py to template_name.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/hbase_to_gcs.py to hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/bigquery_to_gcs.py to bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_bigquery.py to hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_gcs.py to hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/text_to_bigquery.py to text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigquery.py to gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_mongo.py to gcs_to_mongo.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_gcs.py to gcs_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigtable.py to gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_jdbc.py to gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/base_template.py to base_template.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/template_constants.py to template_constants.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/tracking.py to tracking.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/argument_parsing.py to argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_jdbc.py to jdbc_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/jdbc/jdbc_to_gcs.py to jdbc_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/mongo/mongo_to_gcs.py to mongo_to_gcs.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/dataproc_templates-0.0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Will rename output .egg file from dist/dataproc_templates-0.0.1-py3.7.egg to dist/dataproc_templates_distribution.egg\n"
     ]
    }
   ],
   "source": [
    "%cd $WORKING_DIRECTORY\n",
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cdcd5-0f3e-4f51-aa78-93d1976cb0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://main.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  4.2 KiB/  4.2 KiB]                                                \n",
      "Operation completed over 1 objects/4.2 KiB.                                      \n",
      "Copying file://dist/dataproc_templates_distribution.egg [Content-Type=application/octet-stream]...\n",
      "/ [1 files][114.0 KiB/114.0 KiB]                                                \n",
      "Operation completed over 1 objects/114.0 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "! gsutil cp -r $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/dist/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9bb170-09c4-40d1-baaf-9e907f215889",
   "metadata": {},
   "source": [
    "### Step 6: Calculate Parallel Job for MySQL to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c501db0-c1fb-4a05-88b8-a7e546e2b1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of tables for execution : \n",
      "[['CHARACTER_SETS', 'COLLATIONS'], ['COLLATION_CHARACTER_SET_APPLICABILITY', 'COLUMNS'], ['TABLES']]\n"
     ]
    }
   ],
   "source": [
    "# calculate parallel jobs:\n",
    "COMPLETE_LIST = copy.deepcopy(TABLE_LIST)\n",
    "PARALLEL_JOBS = len(TABLE_LIST)//MAX_PARALLELISM\n",
    "JOB_LIST = []\n",
    "while len(COMPLETE_LIST) > 0:\n",
    "    SUB_LIST = []\n",
    "    for i in range(MAX_PARALLELISM):\n",
    "        if len(COMPLETE_LIST)>0 :\n",
    "            SUB_LIST.append(COMPLETE_LIST[0])\n",
    "            COMPLETE_LIST.pop(0)\n",
    "        else:\n",
    "            break\n",
    "    JOB_LIST.append(SUB_LIST)\n",
    "print(\"list of tables for execution : \")\n",
    "print(JOB_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6f83b-891a-4515-a1d6-f3406a25dc2a",
   "metadata": {},
   "source": [
    "### Step 7: Execute Pipeline to Migrate tables from MySQL to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96d67895-ba8e-4876-b929-427d1ea7c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dist/dataproc_templates_distribution.egg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863fa2d8-4ef7-4722-87c8-eec6c06f892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_mysql(EXECUTION_LIST):\n",
    "    EXECUTION_LIST = EXECUTION_LIST\n",
    "    aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "\n",
    "    @dsl.pipeline(\n",
    "        name=\"mysql-to-gcs-pyspark\",\n",
    "        description=\"Pipeline to get data from mysql to gcs\",\n",
    "    )\n",
    "    def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        location: str = REGION,\n",
    "        main_python_file_uri: str = MAIN_PYTHON_FILE,\n",
    "        python_file_uris: list = PYTHON_FILE_URIS,\n",
    "        jar_file_uris: list = JARS,\n",
    "        subnetwork_uri: str = SUBNET\n",
    "    ):\n",
    "        for table in EXECUTION_LIST:\n",
    "            BATCH_ID = \"mysql2gcs-{}\".format(datetime.now().strftime(\"%s\"))\n",
    "            TEMPLATE_SPARK_ARGS = [\n",
    "            \"--template=JDBCTOGCS\",\n",
    "            \"--jdbctogcs.input.url={}\".format(JDBC_URL),\n",
    "            \"--jdbctogcs.input.driver={}\".format(JDBC_DRIVER),\n",
    "            \"--jdbctogcs.input.table={}\".format(table),\n",
    "            \"--jdbctogcs.output.location={}/{}\".format(MYSQL_OUTPUT_GCS_LOCATION,table.lower()),\n",
    "            \"--jdbctogcs.output.mode={}\".format(MYSQL_OUTPUT_GCS_MODE),\n",
    "            \"--jdbctogcs.output.format={}\".format(MYSQL_OUTPUT_GCS_FORMAT)\n",
    "            ]\n",
    "            print(TEMPLATE_SPARK_ARGS)\n",
    "            _ = DataprocPySparkBatchOp(\n",
    "                project=project_id,\n",
    "                location=location,\n",
    "                batch_id=BATCH_ID,\n",
    "                main_python_file_uri=main_python_file_uri,\n",
    "                python_file_uris=python_file_uris,\n",
    "                jar_file_uris=jar_file_uris,\n",
    "                subnetwork_uri=subnetwork_uri,\n",
    "                args=TEMPLATE_SPARK_ARGS,\n",
    "            )\n",
    "            time.sleep(3)\n",
    "\n",
    "    compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "    pipeline = aiplatform.PipelineJob(\n",
    "            display_name=\"pipeline\",\n",
    "            template_path=\"pipeline.json\",\n",
    "            pipeline_root=PIPELINE_ROOT,\n",
    "            enable_caching=False,\n",
    "            )\n",
    "    pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44205b54-1ac7-42f3-85ad-5b20f531056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHARACTER_SETS', 'COLLATIONS']\n",
      "['--template=JDBCTOGCS', '--jdbctogcs.input.url=jdbc:mysql://10.203.209.12:3306/INFORMATION_SCHEMA?user=root&password=naveen', '--jdbctogcs.input.driver=com.mysql.cj.jdbc.Driver', '--jdbctogcs.input.table=CHARACTER_SETS', '--jdbctogcs.output.location=gs://python-dataproc-templates/mysql-gcs-output/character_sets', '--jdbctogcs.output.mode=overwrite', '--jdbctogcs.output.format=csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1295: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--template=JDBCTOGCS', '--jdbctogcs.input.url=jdbc:mysql://10.203.209.12:3306/INFORMATION_SCHEMA?user=root&password=naveen', '--jdbctogcs.input.driver=com.mysql.cj.jdbc.Driver', '--jdbctogcs.input.table=COLLATIONS', '--jdbctogcs.output.location=gs://python-dataproc-templates/mysql-gcs-output/collations', '--jdbctogcs.output.mode=overwrite', '--jdbctogcs.output.format=csv']\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/mysql-to-gcs-pyspark-20220823225931\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/617357862702/locations/us-central1/pipelineJobs/mysql-to-gcs-pyspark-20220823225931')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/mysql-to-gcs-pyspark-20220823225931?project=617357862702\n"
     ]
    }
   ],
   "source": [
    "for execution_list in JOB_LIST:\n",
    "    print(execution_list)\n",
    "    migrate_mysql(execution_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc269506-e87e-4a9d-98e4-c22cfbcddb0b",
   "metadata": {},
   "source": [
    "4. Identify current primary key column name, and partitioned read properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c02cf5-0563-4a37-9c28-b07698ccef6b",
   "metadata": {},
   "source": [
    "5. Should generate logic for partitioned read, such that each partition read is <2 GB in size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c1884-a975-4c7e-8ec5-b34016fb16c1",
   "metadata": {},
   "source": [
    "7. create spanner schema if it does not exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be084c-1ef2-4893-a23f-c00811654871",
   "metadata": {},
   "source": [
    "8. get gcs to spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a81c31-c602-4e35-814a-4f74e7fcc447",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_SPARK_ARGS = [\n",
    "\"--template=JDBCTOGCS\",\n",
    "\"--jdbctogcs.input.url={}\".format(JDBC_URL),\n",
    "\"--jdbctogcs.input.driver={}\".format(JDBC_DRIVER),\n",
    "\"--jdbctogcs.input.table={}\".format(),\n",
    "\"--jdbctogcs.output.location={}/{}\".format(MYSQL_OUTPUT_GCS_LOCATION),\n",
    "\"--jdbctogcs.output.mode={}\".format(MYSQL_OUTPUT_GCS_MODE),\n",
    "\"--jdbctogcs.output.format={}\".format(MYSQL_OUTPUT_GCS_FORMAT)\n",
    "]\n",
    "# --jdbctogcs.input.partitioncolumn=\"id\" \\\n",
    "# --jdbctogcs.input.lowerbound=\"11\" \\\n",
    "# --jdbctogcs.input.upperbound=\"20\" \\\n",
    "# --jdbctogcs.numpartitions=\"4\" \\\n",
    "# --jdbctogcs.output.partitioncolumn=\"department_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fc60f-5cad-4a11-afb2-9d67250bf787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my sql to gcs pending\n",
    "#3.Identify current primary key column name, and partitioned read properties. \n",
    "#4.Should generate logic for partitioned read, such that each partition read is <2 GB in size\n",
    "# lower bound, upper bound partition\n",
    "# parallel execution"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
