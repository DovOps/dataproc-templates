{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9302b3f8-7880-4acd-badd-3f24cfb69ec6",
   "metadata": {},
   "source": [
    "#### Step 1:\n",
    "#### Set Google Cloud properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1eefd-bdf4-4af9-8b6c-213460303b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Configuration\n",
    "# User Inputs\n",
    "\n",
    "get_project_id = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = get_project_id[0]\n",
    "REGION = \"\"  # example\"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://<bucket_name>\" # example \"gs://bucket_name\"\n",
    "SUBNET = \"\" # example \"projects/<project-id>/regions/<region-id>/subnetworks/<subnet-name>\" \n",
    "INPUT_HIVE_DATABASE= \"\"\n",
    "INPUT_HIVE_TABLES= \"\" # example \"table1,table2,table3...\" or \"*\"\n",
    "OUTPUT_BIGQUERY_DATASET= \"\"\n",
    "TEMP_BUCKET= \"<bucket_name>\"\n",
    "HIVE_METASTORE= \"\" # example \"thrift://hive-cluster-m:9083\"\n",
    "MAX_PARALLELISM=10 # Overwrite the value if you want to increase number of parallel Dataproc Batch Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390bae0-ed4b-4605-9bc9-27fcde75f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7696ef-61d7-4c79-9eb2-281079b3802e",
   "metadata": {},
   "source": [
    "# Run Dataproc Templates from Vertex AI Pipelines\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook shows how to build a Vertex AI Pipeline to run a Dataproc Template using the DataprocPySparkBatchOp component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94dbe6-3ea2-4e26-90db-0da6c5a008ab",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html)\n",
    "- [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "- [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "- [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account.  \n",
    "Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd82da5-c46e-4067-b810-93ede117cdda",
   "metadata": {},
   "source": [
    "#### Permissions\n",
    "#### TODO - Discuss the permissions required and specific resources\n",
    "\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "- roles/aiplatform.serviceAgent\n",
    "- roles/aiplatform.customCodeServiceAgent\n",
    "- roles/storage.objectCreator\n",
    "- roles/storage.objectViewer\n",
    "- roles/dataproc.editor\n",
    "- roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2de52d-7099-4f03-becc-9886fa82b130",
   "metadata": {},
   "source": [
    "#### Step 2:\n",
    "#### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3742b9a-143d-49fc-b43c-e56179c7f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Cloud notebooks requires dependencies to be installed with '--user'\n",
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5cc9ee-1137-4fb9-9a7c-a49cd8bc1ae1",
   "metadata": {},
   "source": [
    "### Once you've installed the additional packages, you may need to restart the notebook kernel so it can find the packages.\n",
    "\n",
    "Uncomment & Run this cell if you want to restart the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c19b5e-e7d9-416f-ad12-edc19b6877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#    import IPython\n",
    "#    app = IPython.Application.instance()\n",
    "#    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1af72a-1fcc-4495-b710-58eb950a1d45",
   "metadata": {},
   "source": [
    "#### Step 3:\n",
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aede36-3a0a-43f7-8e4c-db2d8087e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7aa35-f539-4f6a-a243-3e0799c47499",
   "metadata": {},
   "source": [
    "#### Step 4:\n",
    "#### Change working directory to the Dataproc Templates python folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0e80a-e8b7-4e54-9f99-b7874e164978",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY = \"/home/jupyter/dataproc-templates/python\"\n",
    "%cd /home/jupyter/dataproc-templates/python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac30fae-1583-462e-a7a9-61f2736dbc5a",
   "metadata": {},
   "source": [
    "#### Step 5:\n",
    "#### Build Dataproc Templates python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa8c93-5e69-48fc-984a-f4fa3b28519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_EGG_FILE = \"dist/dataproc_templates_distribution.egg\"\n",
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f7ad9-7d01-4496-b08c-f9e69775d0c1",
   "metadata": {},
   "source": [
    "#### Step 6:\n",
    "#### Copy package to the GCS bucket\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/storage.objectCreator\n",
    " - roles/storage.objectViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd85ce-46f6-4b35-8997-2189cf1b2eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "! gsutil cp -r $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/dist/\n",
    "! gsutil cp dataproc_templates/hive/get_hive_tables.py $GCS_STAGING_LOCATION/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33b841-0538-491f-b686-38ab4a01ad1e",
   "metadata": {},
   "source": [
    "#### Step 7:\n",
    "#### Choose template and set template arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18574192-8d4d-43c8-98e5-f3d613c98fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dist/dataproc_templates_distribution.egg\"]\n",
    "JARS = [\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"]\n",
    "GET_HIVE_TABLES_PY=GCS_STAGING_LOCATION + \"/get_hive_tables.py\"\n",
    "\n",
    "TEMPLATE_SPARK_ARGS = [\n",
    "\"--template=HIVETOBIGQUERY\",\n",
    "\"--hive.bigquery.input.database={}\".format(INPUT_HIVE_DATABASE),\n",
    "\"--hive.bigquery.input.table={}\".format(INPUT_HIVE_TABLES),\n",
    "\"--hive.bigquery.output.dataset={}\".format(OUTPUT_BIGQUERY_DATASET),\n",
    "\"--hive.bigquery.output.mode=overwrite\",\n",
    "\"--hive.bigquery.temp.bucket.name={}\".format(TEMP_BUCKET)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676c73f-7b70-482c-87ae-e5471c58fa03",
   "metadata": {},
   "source": [
    "#### Step 8:\n",
    "#### Build pipeline and run Dataproc Template on Vertex AI Pipelines to get list of all the Hive Tables\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/dataproc.editor\n",
    " - roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd45a7-85f9-4c2f-b8f5-07f09f21fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch ID should be  4-63 characters\n",
    "BATCH_ID = \"b-\"+INPUT_HIVE_DATABASE+\"-\"+ datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "runtime_prop={}\n",
    "runtime_prop['spark.hadoop.hive.metastore.uris']=HIVE_METASTORE\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataproc-templates-pyspark\",\n",
    "    description=\"DataprocPySparkBatchOp to get list of tables from hive metastore: \"+HIVE_METASTORE,\n",
    ")\n",
    "def pipeline(\n",
    "    batch_id: str = BATCH_ID,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    main_python_file_uri: str = GET_HIVE_TABLES_PY,\n",
    "    python_file_uris: list = PYTHON_FILE_URIS,\n",
    "    jar_file_uris: list = JARS,\n",
    "    subnetwork_uri: str = SUBNET,\n",
    "    args: list = TEMPLATE_SPARK_ARGS,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    _ = DataprocPySparkBatchOp(\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "            batch_id=BATCH_ID,\n",
    "            main_python_file_uri=main_python_file_uri,\n",
    "            python_file_uris=python_file_uris,\n",
    "            jar_file_uris=jar_file_uris,\n",
    "            runtime_config_properties=runtime_prop,       \n",
    "            subnetwork_uri=subnetwork_uri,\n",
    "            args=args\n",
    "        )\n",
    "        \n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171c548-8c7d-4d81-a4fd-2b0191eb589b",
   "metadata": {},
   "source": [
    "#### Step 9:\n",
    "#### Copy Hive Tables File from GCS to Local\n",
    "\n",
    "The above dataproc batch job will bring in all the table names from hive metastore and save in a file present in notebooks/HIVE/tables folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bcd941-2bc6-4471-8889-6d56fc46cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "table_file=WORKING_DIRECTORY+\"/notebooks/HIVE/tables/{}.csv\".format(BATCH_ID)\n",
    "in_file='gs://'+TEMP_BUCKET+'/'+INPUT_HIVE_DATABASE+'/*.csv'\n",
    "os.system(\"gsutil cp {} {}\".format(in_file,table_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eda89f-55bc-43dc-ba03-60048e4efe36",
   "metadata": {},
   "source": [
    "#### Step 10:\n",
    "#### Calculate Split Count\n",
    "\n",
    "Calculate the number of tables to be loaded in each batch job based on MAX_PARALLELISM value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73197165-0ea0-4732-841d-8ba4bcd8a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "tables_f = open(table_file, 'r+')\n",
    "table_list = [line for line in tables_f.readlines()]\n",
    "table_count=len(table_list)\n",
    "split_count=floor((table_count+MAX_PARALLELISM-1)/MAX_PARALLELISM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bcb56-c950-4366-9fdf-a9a6da8defca",
   "metadata": {},
   "source": [
    "#### Step 11:\n",
    "#### Get Table list function\n",
    "\n",
    "Function to split table list based on the number of jobs to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52f2c7-ac61-41d3-a46a-3c740d828947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "def get_table_list(i):\n",
    "    input_hive_tables=table_list[i:split_count+i]\n",
    "    input_hive_tables_string=(','.join(input_hive_tables)).replace('\\n','')\n",
    "    res = ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))\n",
    "    TEMPLATE_SPARK_ARGS = [\n",
    "                            \"--template=HIVETOBIGQUERY\",\n",
    "                            \"--hive.bigquery.input.database={}\".format(INPUT_HIVE_DATABASE),\n",
    "                            \"--hive.bigquery.input.table={}\".format(input_hive_tables),\n",
    "                            \"--hive.bigquery.output.dataset={}\".format(OUTPUT_BIGQUERY_DATASET),\n",
    "                            \"--hive.bigquery.output.mode=overwrite\",\n",
    "                            \"--hive.bigquery.temp.bucket.name={}\".format(TEMP_BUCKET),\n",
    "                            \"--hive.database.all.tables={}\".format(input_hive_tables_string),\n",
    "                            \"--migration_id={}\".format(res)        \n",
    "                          ]\n",
    "    return TEMPLATE_SPARK_ARGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896c320-f73e-40e8-9aab-cb519de241f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 12:\n",
    "#### Build pipeline and run Dataproc Template on Vertex AI Pipelines to migrate HIVE tables to BigQuery\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/dataproc.editor\n",
    " - roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb58c14-c691-4e7f-982c-fb74f4820b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_prop={}\n",
    "runtime_prop['spark.hadoop.hive.metastore.uris']=HIVE_METASTORE\n",
    "runtime_prop['mapreduce.fileoutputcommitter.marksuccessfuljobs'] = \"false\"\n",
    "\n",
    "import time\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataproc-templates-pyspark\",\n",
    "    description=\"DataprocPySparkBatchOp to run HiveToBigQuery PySpark Dataproc Template batch workload\",\n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    batch_id: str = BATCH_ID,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    main_python_file_uri: str = MAIN_PYTHON_FILE,\n",
    "    python_file_uris: list = PYTHON_FILE_URIS,\n",
    "    jar_file_uris: list = JARS,\n",
    "    subnetwork_uri: str = SUBNET,\n",
    "    args: list = TEMPLATE_SPARK_ARGS,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    i=0\n",
    "    while i<table_count:\n",
    "        args=get_table_list(i)\n",
    "        i=i+split_count\n",
    "        BATCH_ID = \"b-\"+INPUT_HIVE_DATABASE+\"-\"+ datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        _ = DataprocPySparkBatchOp(\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "            batch_id=BATCH_ID,\n",
    "            main_python_file_uri=main_python_file_uri,\n",
    "            python_file_uris=python_file_uris,\n",
    "            jar_file_uris=jar_file_uris,\n",
    "            runtime_config_properties=runtime_prop,       \n",
    "            subnetwork_uri=subnetwork_uri,\n",
    "            args=args\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")  \n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95249f-c647-48a9-832f-be127d0add4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
