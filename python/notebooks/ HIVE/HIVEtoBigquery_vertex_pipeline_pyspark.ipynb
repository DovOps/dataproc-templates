{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c62b00-03d1-4d9e-beb2-f9961452338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7696ef-61d7-4c79-9eb2-281079b3802e",
   "metadata": {},
   "source": [
    "# Run Dataproc Templates from Vertex AI Pipelines\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook shows how to build a Vertex AI Pipeline to run a Dataproc Template using the DataprocPySparkBatchOp component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94dbe6-3ea2-4e26-90db-0da6c5a008ab",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "- [DataprocPySparkBatchOp reference](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.experimental.dataproc.html)\n",
    "- [Kubeflow SDK Overview](https://www.kubeflow.org/docs/components/pipelines/sdk/sdk-overview/)\n",
    "- [Dataproc Serverless in Vertex AI Pipelines tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_dataproc_serverless_pipeline_components.ipynb)\n",
    "- [Build a Vertex AI Pipeline](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline)\n",
    "\n",
    "This notebook is built to run a Vertex AI User-Managed Notebook using the default Compute Engine Service Account.  \n",
    "Check the Dataproc Serverless in Vertex AI Pipelines tutorial linked above to learn how to setup a different Service Account.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd82da5-c46e-4067-b810-93ede117cdda",
   "metadata": {},
   "source": [
    "#### Permissions\n",
    "\n",
    "Make sure that the service account used to run the notebook has the following roles:\n",
    "\n",
    "- roles/aiplatform.serviceAgent\n",
    "- roles/aiplatform.customCodeServiceAgent\n",
    "- roles/storage.objectCreator\n",
    "- roles/storage.objectViewer\n",
    "- roles/dataproc.editor\n",
    "- roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2de52d-7099-4f03-becc-9886fa82b130",
   "metadata": {},
   "source": [
    "#### Install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3742b9a-143d-49fc-b43c-e56179c7f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Cloud notebooks requires dependencies to be installed with '--user'\n",
    "! pip3 install --upgrade google-cloud-pipeline-components kfp --user -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5cc9ee-1137-4fb9-9a7c-a49cd8bc1ae1",
   "metadata": {},
   "source": [
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c19b5e-e7d9-416f-ad12-edc19b6877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Comment this step\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1af72a-1fcc-4495-b710-58eb950a1d45",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7aede36-3a0a-43f7-8e4c-db2d8087e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7aa35-f539-4f6a-a243-3e0799c47499",
   "metadata": {},
   "source": [
    "#### Change working directory to the Dataproc Templates python folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e0e80a-e8b7-4e54-9f99-b7874e164978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dataproc-templates/python\n"
     ]
    }
   ],
   "source": [
    "WORKING_DIRECTORY = \"dataproc-templates/python/\"\n",
    "%cd /home/jupyter/dataproc-templates/python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc50fd-c262-4308-804e-298e62af14b4",
   "metadata": {},
   "source": [
    "#### Set Google Cloud properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f010bef0-2d9f-430a-b89c-1875b1647c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Inputs\n",
    "\n",
    "get_project_id = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID = get_project_id[0]\n",
    "REGION = \"us-west1\"\n",
    "GCS_STAGING_LOCATION = \"gs://shubham_bqtest\"\n",
    "SUBNET = \"projects/yadavaja-sandbox/regions/us-west1/subnetworks/test-subnet1\"\n",
    "input_hive_database=\"hive2bq\"\n",
    "input_hive_tables=\"empdata\"\n",
    "output_bigquery_dataset=\"hive2bq\"\n",
    "temp_bucket=\"shubham_bqtest\"\n",
    "hive_metastore=\"thrift://hive-cluster-m:9083\"\n",
    "max_parallelism=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac30fae-1583-462e-a7a9-61f2736dbc5a",
   "metadata": {},
   "source": [
    "#### Build Dataproc Templates python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfaa8c93-5e69-48fc-984a-f4fa3b28519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_egg\n",
      "running egg_info\n",
      "writing dataproc_templates.egg-info/PKG-INFO\n",
      "writing dependency_links to dataproc_templates.egg-info/dependency_links.txt\n",
      "writing requirements to dataproc_templates.egg-info/requires.txt\n",
      "writing top-level names to dataproc_templates.egg-info/top_level.txt\n",
      "reading manifest file 'dataproc_templates.egg-info/SOURCES.txt'\n",
      "writing manifest file 'dataproc_templates.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "/opt/conda/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/test\n",
      "creating build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/__init__.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "copying build/lib/test/util/test_argument_parsing.py -> build/bdist.linux-x86_64/egg/test/util\n",
      "creating build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/__init__.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "copying build/lib/test/hive/test_hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/hive\n",
      "creating build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/__init__.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_text_to_bigquery.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "copying build/lib/test/gcs/test_gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/test/gcs\n",
      "creating build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/__init__.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "copying build/lib/test/hbase/test_hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/test/hbase\n",
      "creating build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "copying build/lib/test/bigquery/test_bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/test/bigquery\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/tracking.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/argument_parsing.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "copying build/lib/dataproc_templates/util/template_constants.py -> build/bdist.linux-x86_64/egg/dataproc_templates/util\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/get_hive_tables.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/hive/hive_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hive\n",
      "copying build/lib/dataproc_templates/template_name.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_jdbc.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigtable.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/text_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/gcs/gcs_to_bigquery.py -> build/bdist.linux-x86_64/egg/dataproc_templates/gcs\n",
      "copying build/lib/dataproc_templates/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "copying build/lib/dataproc_templates/base_template.py -> build/bdist.linux-x86_64/egg/dataproc_templates\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "copying build/lib/dataproc_templates/hbase/hbase_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/hbase\n",
      "creating build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/bigquery_to_gcs.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "copying build/lib/dataproc_templates/bigquery/__init__.py -> build/bdist.linux-x86_64/egg/dataproc_templates/bigquery\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/util/test_argument_parsing.py to test_argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_gcs.py to test_hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hive/test_hive_to_bigquery.py to test_hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigquery.py to test_gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_text_to_bigquery.py to test_text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_jdbc.py to test_gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/gcs/test_gcs_to_bigtable.py to test_gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/hbase/test_hbase_to_gcs.py to test_hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/test/bigquery/test_bigquery_to_gcs.py to test_bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/tracking.py to tracking.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/argument_parsing.py to argument_parsing.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/util/template_constants.py to template_constants.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/get_hive_tables.py to get_hive_tables.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_gcs.py to hive_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hive/hive_to_bigquery.py to hive_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/template_name.py to template_name.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_jdbc.py to gcs_to_jdbc.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigtable.py to gcs_to_bigtable.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/text_to_bigquery.py to text_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/gcs/gcs_to_bigquery.py to gcs_to_bigquery.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/base_template.py to base_template.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/hbase/hbase_to_gcs.py to hbase_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/bigquery_to_gcs.py to bigquery_to_gcs.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/dataproc_templates/bigquery/__init__.py to __init__.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying dataproc_templates.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "creating 'dist/dataproc_templates-0.0.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Will rename output .egg file from dist/dataproc_templates-0.0.1-py3.7.egg to dist/dataproc_templates_distribution.egg\n"
     ]
    }
   ],
   "source": [
    "PACKAGE_EGG_FILE = \"dist/dataproc_templates_distribution.egg\"\n",
    "! python ./setup.py bdist_egg --output=$PACKAGE_EGG_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f7ad9-7d01-4496-b08c-f9e69775d0c1",
   "metadata": {},
   "source": [
    "#### Copy package to the GCS bucket\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/storage.objectCreator\n",
    " - roles/storage.objectViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33dd85ce-46f6-4b35-8997-2189cf1b2eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://main.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  3.6 KiB/  3.6 KiB]                                                \n",
      "Operation completed over 1 objects/3.6 KiB.                                      \n",
      "Copying file://dist/dataproc_templates_distribution.egg [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 77.2 KiB/ 77.2 KiB]                                                \n",
      "Operation completed over 1 objects/77.2 KiB.                                     \n",
      "Copying file://dataproc_templates/hive/get_hive_tables.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
      "Operation completed over 1 objects/3.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp main.py $GCS_STAGING_LOCATION/\n",
    "! gsutil cp -r $PACKAGE_EGG_FILE $GCS_STAGING_LOCATION/dist/\n",
    "! gsutil cp dataproc_templates/hive/get_hive_tables.py $GCS_STAGING_LOCATION/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6d386-e37e-4f7e-8376-f0f0ee6861a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set Dataproc Templates properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca0ee78-2690-4358-b684-47453b382743",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ROOT = GCS_STAGING_LOCATION + \"/pipeline_root/dataproc_pyspark\"\n",
    "MAIN_PYTHON_FILE = GCS_STAGING_LOCATION + \"/main.py\"\n",
    "PYTHON_FILE_URIS = [GCS_STAGING_LOCATION + \"/dist/dataproc_templates_distribution.egg\"]\n",
    "JARS = [\"gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar\"]\n",
    "GET_HIVE_TABLES_PY=GCS_STAGING_LOCATION + \"/get_hive_tables.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d33b841-0538-491f-b686-38ab4a01ad1e",
   "metadata": {},
   "source": [
    "#### Choose template and set template arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6dcb79-3a71-4cd5-a67e-1783d2456545",
   "metadata": {},
   "source": [
    "GCSTOBIGQUERY is chosen in this notebook as an example.  \n",
    "Check the arguments in the template's documentation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18574192-8d4d-43c8-98e5-f3d613c98fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_SPARK_ARGS = [\n",
    "\"--template=HIVETOBIGQUERY\",\n",
    "\"--hive.bigquery.input.database={}\".format(input_hive_database),\n",
    "\"--hive.bigquery.input.table={}\".format(input_hive_tables),\n",
    "\"--hive.bigquery.output.dataset={}\".format(output_bigquery_dataset),\n",
    "\"--hive.bigquery.output.mode=overwrite\",\n",
    "\"--hive.bigquery.temp.bucket.name={}\".format(temp_bucket)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676c73f-7b70-482c-87ae-e5471c58fa03",
   "metadata": {},
   "source": [
    "Get List of All the Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4dd45a7-85f9-4c2f-b8f5-07f09f21fac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1295: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dataproc-templates-pyspark-20220731130011?project=617357862702\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220731130011\n"
     ]
    }
   ],
   "source": [
    "# Batch ID should be  4-63 characters\n",
    "BATCH_ID = \"b-\"+input_hive_database+\"-\"+ datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "runtime_prop={}\n",
    "runtime_prop['spark.hadoop.hive.metastore.uris']=hive_metastore\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataproc-templates-pyspark\",\n",
    "    description=\"DataprocPySparkBatchOp to get list of tables from hive metastore: \"+hive_metastore,\n",
    ")\n",
    "def pipeline(\n",
    "    batch_id: str = BATCH_ID,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    main_python_file_uri: str = GET_HIVE_TABLES_PY,\n",
    "    python_file_uris: list = PYTHON_FILE_URIS,\n",
    "    jar_file_uris: list = JARS,\n",
    "    subnetwork_uri: str = SUBNET,\n",
    "    args: list = TEMPLATE_SPARK_ARGS,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    _ = DataprocPySparkBatchOp(\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "            batch_id=BATCH_ID,\n",
    "            main_python_file_uri=main_python_file_uri,\n",
    "            python_file_uris=python_file_uris,\n",
    "            jar_file_uris=jar_file_uris,\n",
    "            runtime_config_properties=runtime_prop,       \n",
    "            subnetwork_uri=subnetwork_uri,\n",
    "            args=args\n",
    "        )\n",
    "        \n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171c548-8c7d-4d81-a4fd-2b0191eb589b",
   "metadata": {},
   "source": [
    "# Copy Hive Tables File from GCS to Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79bcd941-2bc6-4471-8889-6d56fc46cca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://shubham_bqtest/hive2bq/part-00000-359f375a-7477-4ec5-a13c-aaf55e9594d2-c000.csv...\n",
      "/ [1 files][   91.0 B/   91.0 B]                                                \n",
      "Operation completed over 1 objects/91.0 B.                                       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "table_file=WORKING_DIRECTORY+\"/notebooks/HIVE/tables/{}.csv\".format(BATCH_ID)\n",
    "in_file='gs://'+temp_bucket+'/'+input_hive_database+'/*.csv'\n",
    "os.system(\"gsutil cp {} {}\".format(in_file,table_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c07c93b1-4918-4b83-bde1-87ec1c3638a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/dataproc-templates/python/\n"
     ]
    }
   ],
   "source": [
    "print(WORKING_DIRECTORY)\n",
    "new-dataproc-templates/dataproc-templates/python/notebooks/HIVE/HIVEtoBigquery_vertex_pipeline_pyspark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eda89f-55bc-43dc-ba03-60048e4efe36",
   "metadata": {},
   "source": [
    "# Calculate the number of jobs to run based on MAX_Parallelism value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73197165-0ea0-4732-841d-8ba4bcd8a94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import *\n",
    "tables_f = open(table_file, 'r+')\n",
    "table_list = [line for line in tables_f.readlines()]\n",
    "table_count=len(table_list)\n",
    "result=floor((table_count+max_parallelism-1)/max_parallelism)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bcb56-c950-4366-9fdf-a9a6da8defca",
   "metadata": {},
   "source": [
    "# Split tables based on the number of jobs to run (calculated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b52f2c7-ac61-41d3-a46a-3c740d828947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "def get_table_list(i):\n",
    "    input_hive_tables=table_list[i:result+i]\n",
    "    input_hive_tables_string=(','.join(input_hive_tables)).replace('\\n','')\n",
    "    res = ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))\n",
    "    TEMPLATE_SPARK_ARGS = [\n",
    "\"--template=HIVETOBIGQUERY\",\n",
    "\"--hive.bigquery.input.database={}\".format(input_hive_database),\n",
    "\"--hive.bigquery.input.table={}\".format(input_hive_tables),\n",
    "\"--hive.bigquery.output.dataset={}\".format(output_bigquery_dataset),\n",
    "\"--hive.bigquery.output.mode=overwrite\",\n",
    "\"--hive.bigquery.temp.bucket.name={}\".format(temp_bucket),\n",
    "\"--hive.database.all.tables={}\".format(input_hive_tables_string),\n",
    "\"--migration_id={}\".format(res)        \n",
    "]\n",
    "    return TEMPLATE_SPARK_ARGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896c320-f73e-40e8-9aab-cb519de241f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Build pipeline and run Dataproc Template on Vertex AI Pipelines\n",
    "\n",
    "For this, make sure that the service account used to run the notebook has the following roles:\n",
    " - roles/dataproc.editor\n",
    " - roles/dataproc.worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfb58c14-c691-4e7f-982c-fb74f4820b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/dataproc-templates-pyspark-20220729044905?project=617357862702\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/617357862702/locations/us-central1/pipelineJobs/dataproc-templates-pyspark-20220729044905\n"
     ]
    }
   ],
   "source": [
    "runtime_prop={}\n",
    "runtime_prop['spark.hadoop.hive.metastore.uris']=hive_metastore\n",
    "runtime_prop['mapreduce.fileoutputcommitter.marksuccessfuljobs'] = \"false\"\n",
    "\n",
    "import time\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=GCS_STAGING_LOCATION)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataproc-templates-pyspark\",\n",
    "    description=\"DataprocPySparkBatchOp to run HiveToBigQuery PySpark Dataproc Template batch workload\",\n",
    ")\n",
    "\n",
    "def pipeline(\n",
    "    batch_id: str = BATCH_ID,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = REGION,\n",
    "    main_python_file_uri: str = MAIN_PYTHON_FILE,\n",
    "    python_file_uris: list = PYTHON_FILE_URIS,\n",
    "    jar_file_uris: list = JARS,\n",
    "    subnetwork_uri: str = SUBNET,\n",
    "    args: list = TEMPLATE_SPARK_ARGS,\n",
    "):\n",
    "    from google_cloud_pipeline_components.experimental.dataproc import \\\n",
    "        DataprocPySparkBatchOp\n",
    "\n",
    "    i=0\n",
    "    while i<table_count:\n",
    "        args=get_table_list(i)\n",
    "        i=i+result\n",
    "        BATCH_ID = \"b-\"+input_hive_database+\"-\"+ datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        _ = DataprocPySparkBatchOp(\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "            batch_id=BATCH_ID,\n",
    "            main_python_file_uri=main_python_file_uri,\n",
    "            python_file_uris=python_file_uris,\n",
    "            jar_file_uris=jar_file_uris,\n",
    "            runtime_config_properties=runtime_prop,       \n",
    "            subnetwork_uri=subnetwork_uri,\n",
    "            args=args\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline.json\")\n",
    "\n",
    "pipeline = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")  \n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95249f-c647-48a9-832f-be127d0add4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
